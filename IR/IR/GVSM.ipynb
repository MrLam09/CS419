{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kết nối GDrive"
      ],
      "metadata": {
        "id": "arxA4LRf-Eu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/GDrive\")"
      ],
      "metadata": {
        "id": "3GGc4AJPSaR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df8c169-3de2-492a-ebdf-abeef15cf6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/GDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import thư viện"
      ],
      "metadata": {
        "id": "hUzk-4AT-Lhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import * \n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import math\n",
        "import operator\n",
        "import string\n",
        "import nltk\n",
        "from collections import Counter,defaultdict\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "stop_word = stopwords.words('english')\n"
      ],
      "metadata": {
        "id": "SPkscS4eSnNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb2f734-e561-409d-ccff-e387937ebabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiền xử lý data"
      ],
      "metadata": {
        "id": "ySwtWoGy-QPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "  # chữ cái thường\n",
        "  data = data.lower()\n",
        "  # loại bỏ punctuation và ký tự đặc biệt\n",
        "  data = data.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "  data = data.translate(str.maketrans(\"‘’’–——−\", '       '))\n",
        "  data = re.sub('\\n',' ',data)\n",
        "  data = re.sub('[^A-Za-z]',' ',data)\n",
        "  return data"
      ],
      "metadata": {
        "id": "ZlMXFlrlL82v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "text = []\n",
        "path_fold = '/content/GDrive/MyDrive/test_small'\n",
        "listfName = os.listdir(path_fold)\n",
        "\n",
        "for i in range(len(listfName)):\n",
        "  f = open(os.path.join(path_fold, listfName[i]), 'r', encoding='UTF-8')\n",
        "  data = f.read()\n",
        "  data = preprocess_data(data)\n",
        "  text.append(data)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "805cyt9ISsX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_word(data):\n",
        "  return word_tokenize(data)\n",
        "def remove_stopwords(data):\n",
        "  list_filter = []\n",
        "  for word in data:\n",
        "    if word not in stop_word:\n",
        "      list_filter.append(word)\n",
        "  return list_filter\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "def lemma_stem(action,data):\n",
        "  if action == 1:\n",
        "      stemmed = []\n",
        "      for i in data:\n",
        "        stemmed.append(stemmer.stem(i))\n",
        "      return stemmed\n",
        "  elif action == 0:\n",
        "      lemmatized = []\n",
        "      for i in range(0,len(data)):\n",
        "        lemmatized.append(lemmatizer.lemmatize(data[i]))\n",
        "      return lemmatized\n",
        "  else:\n",
        "    list_w = []\n",
        "    for i in range(0,len(data)):\n",
        "        list_w.append(data[i])\n",
        "    return list_w\n",
        "def process(data,action):\n",
        "  list_words = []\n",
        "  for i in range(0,len(data)):\n",
        "    text = split_word(data[i])\n",
        "    text = remove_stopwords(text) \n",
        "    result = []\n",
        "    if action == 1:\n",
        "      for i in text:\n",
        "          result.append(stemmer.stem(i))\n",
        "    else:\n",
        "      for i in text:\n",
        "          result.append(stemmer.stem(i))\n",
        "    list_words.append(result)\n",
        "  return list_words"
      ],
      "metadata": {
        "id": "h2uu_lshS3ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_words = process(text,0)"
      ],
      "metadata": {
        "id": "8Qv1sny9S-6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lập chỉ mục đảo ngược"
      ],
      "metadata": {
        "id": "Pyx5maWC-cxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "data = []\n",
        "fred = dict()\n",
        "for id_doc in range(0,len(list_words)):\n",
        "  for j in Counter(list_words[id_doc]):\n",
        "      counter =[]\n",
        "      counter.append(j)\n",
        "      counter.append((id_doc+1,Counter(list_words[id_doc])[j]))\n",
        "      counter.append(Counter(list_words[id_doc])[j])\n",
        "      if j not in fred:\n",
        "        fred[j] = Counter(list_words[id_doc])[j]\n",
        "      else:\n",
        "        fred[j] += Counter(list_words[id_doc])[j]\n",
        "      data.append(counter)\n",
        "unique_words = defaultdict(list)\n",
        "for line in data:\n",
        "  unique_words[line[0]].append(line[1])\n",
        "List = []\n",
        "for item in unique_words:\n",
        "   line = []\n",
        "   line.append(item)\n",
        "   line.append(unique_words[item])\n",
        "   line.append(fred[item])\n",
        "   line.append(len(unique_words[item]))\n",
        "   List.append(line)\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "AuFQpP2mTtRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tạo dictionary tần số term trong 1 document"
      ],
      "metadata": {
        "id": "6cZUUE8D-nfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def termFrequencyInDoc(List):\n",
        "    termFreqADoc=dict()\n",
        "    for line in List:    \n",
        "      term = line[0] \n",
        "      for id_doc,count in line[1]:\n",
        "        if id_doc not in termFreqADoc:\n",
        "            termFreqADoc[id_doc] = {}\n",
        "            termFreqADoc[id_doc][term] = count\n",
        "        else:\n",
        "           termFreqADoc[id_doc][term] = count\n",
        "    return termFreqADoc"
      ],
      "metadata": {
        "id": "FXbfKcOFTywQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "termFreqADoc = termFrequencyInDoc(List)\n",
        "termFreqADoc"
      ],
      "metadata": {
        "id": "F0XFfu4kQjCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30ac027-00ff-4f8e-abcc-c728974be9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'two': 1, 'squar': 1, 'root': 1, 'approxim': 1},\n",
              " 2: {'root': 1,\n",
              "  'extract': 1,\n",
              "  'repeat': 1,\n",
              "  'subtract': 1,\n",
              "  'digit': 1,\n",
              "  'comput': 1},\n",
              " 3: {'techniqu': 1, 'depart': 1, 'matrix': 1, 'program': 1, 'scheme': 1},\n",
              " 4: {'preliminari': 1, 'report': 1, 'intern': 1, 'algebra': 1, 'languag': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GVSM"
      ],
      "metadata": {
        "id": "mDM2ub8P-wTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minterm =[]\n",
        "for doc in termFreqADoc:\n",
        "  cn=0\n",
        "  val=0\n",
        "  for value in termFreqADoc[doc]:\n",
        "    if termFreqADoc[doc][value]>0:\n",
        "      val=val+pow(2,cn)\n",
        "    cn=cn+1\n",
        "  minterm=minterm+[val]\t\t\n",
        "\n",
        "unit_vectors=[]\n",
        "tot_words=len(List)"
      ],
      "metadata": {
        "id": "3VavHBQQhn5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98zJbi6Q-1rE",
        "outputId": "9a86926b-c6eb-46fe-c940-ec7236cfd2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 63, 31, 31]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "document 1 has minterm(15),document 2 has minterm(63),document 3 has minterm(31),document 4 has minterm(31)"
      ],
      "metadata": {
        "id": "LvXS8NCkEo76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p=[]\n",
        "size_of_minterms=pow(2,(tot_words)) #size of GVSM vector space is 2^total_words\n",
        "tmp = [0 for j in range(pow(2,tot_words))]\n",
        "\n",
        "for i in range(0,tot_words):\n",
        "\ttmp_unit_vector = tmp\n",
        "\tcnt=0\n",
        "\tfor doc in termFreqADoc:\n",
        "\t\tcn=0\n",
        "\t\tfor value in termFreqADoc[doc]:\n",
        "\t\t\tif i == cn:\n",
        "\t\t\t\ttmp_unit_vector[minterm[cnt]]=tmp_unit_vector[minterm[cnt]]+termFreqADoc[doc][value]\n",
        "\t\t\t\t#print(tmp_unit_vector[minterm[cnt]])\n",
        "\t\t\tcn=cn+1\t\n",
        "\t\tcnt=cnt+1\n",
        "\n",
        "\tmagnitude=np.linalg.norm(tmp_unit_vector)\n",
        "\tmyArr=np.array(tmp_unit_vector)\n",
        "\tnewArr=myArr/magnitude\n",
        "\tp.append(newArr)\n"
      ],
      "metadata": {
        "id": "7v4xzFt4nbZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em lấy Document 1 làm câu truy vấn, document 2-4 làm tập tài liệu cần tính xếp hạng"
      ],
      "metadata": {
        "id": "m_WsNqiC9sWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "queryVector=np.zeros(pow(2,tot_words))\n",
        "sim=[]\n",
        "count_file=0\n",
        "\n",
        "#this loop constructs document and query vectors in the GVSM vector space as linear combination of minterm vectors and cosine similarity\n",
        "for doc in termFreqADoc:\n",
        "\n",
        "\tdocVector=np.zeros(pow(2,tot_words))\n",
        "\tcount=0 #keeps count of index terms\n",
        "\n",
        "\tfor value in termFreqADoc[doc]:\n",
        "\t\tdocVector+=(termFreqADoc[doc][value]*p[count]) #product of tfidf value from matrix and nomrmalised term vectors from p\n",
        "\t\tcount=count+1\n",
        "\tB=np.array(docVector)\n",
        "\tif(count_file==0):\n",
        "\t\tqueryVector+=docVector #assign query vector\n",
        "\t\tA=np.array(queryVector)\n",
        "\tsim.append(np.dot(A,B)/(norm(A)*norm(B))) #cosine similarity\n",
        "\n",
        "\tcount_file+=1\n",
        "\n",
        "#sort the documents based on their similarity from highest to lowest similarity order\n",
        "sim.sort(reverse=True)\n",
        "\n",
        "count_file=0\n",
        "for i in range(1,4):\n",
        "   print('Doc ',i,': ',text[i])\n",
        "print('Query: ',text[0])\n",
        "print('Ranking by cosine similarity:')\n",
        "#print the documents' ranking\n",
        "for s in sim[1:]:\n",
        "\tprint('docId', count_file+1,' : ',s)\n",
        "\tcount_file+=1"
      ],
      "metadata": {
        "id": "VjQFOrjwakG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c1c9f0-addf-489a-89f9-a2315d6299fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc  1 :  extraction of roots by repeated subtractions for digital computers \n",
            "Doc  2 :  techniques department on matrix program schemes \n",
            "Doc  3 :  preliminary report international algebraic language \n",
            "Query:  two square root approximations \n",
            "Ranking by cosine similarity:\n",
            "docId 1  :  0.9998816776422816\n",
            "docId 2  :  0.9998816776422816\n",
            "docId 3  :  0.9995442822834324\n"
          ]
        }
      ]
    }
  ]
}